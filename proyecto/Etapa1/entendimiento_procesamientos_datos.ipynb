{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías y lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 25) # Número máximo de columnas a mostrar\n",
    "pd.set_option('display.max_rows', 50) # Numero máximo de filas a mostar\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "np.random.seed(3301)\n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sns \n",
    "\n",
    "# Matplolib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# librería Natural Language Toolkit, usada para trabajar con texts\n",
    "import unicodedata\n",
    "import string\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import inflect\n",
    "import contractions\n",
    "import nltk\n",
    "# Punkt permite separar un text en frases.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas:  195700\n",
      "Número de columnas:  3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21738</th>\n",
       "      <td>135566</td>\n",
       "      <td>AAAA I'm literally the stupidest person in exi...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26627</th>\n",
       "      <td>17383</td>\n",
       "      <td>Is it just me or is the sound of rain One of t...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135198</th>\n",
       "      <td>36707</td>\n",
       "      <td>How close i amam tired of living, tired of bei...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140848</th>\n",
       "      <td>186151</td>\n",
       "      <td>Guess who kissed a girl? Not me.\\n\\n&amp;amp;#x200...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79670</th>\n",
       "      <td>255320</td>\n",
       "      <td>Should I delete my Reddit account? I mean shou...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "21738       135566  AAAA I'm literally the stupidest person in exi...   \n",
       "26627        17383  Is it just me or is the sound of rain One of t...   \n",
       "135198       36707  How close i amam tired of living, tired of bei...   \n",
       "140848      186151  Guess who kissed a girl? Not me.\\n\\n&amp;#x200...   \n",
       "79670       255320  Should I delete my Reddit account? I mean shou...   \n",
       "\n",
       "              class  \n",
       "21738   non-suicide  \n",
       "26627   non-suicide  \n",
       "135198      suicide  \n",
       "140848  non-suicide  \n",
       "79670   non-suicide  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_route = '../data/SuicidiosProyecto.csv'\n",
    "df_originales = pd.read_csv(db_route, encoding = 'ISO-8859-1')\n",
    "\n",
    "# Imprimir número de filas\n",
    "print('Número de filas: ', df_originales.shape[0])\n",
    "# Imprimir número de columnas\n",
    "print('Número de columnas: ', df_originales.shape[1])\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_originales.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las primeras pruebas por temas de tiempos de ejecución, se utilizan solo 5000 registros aleatorios. Estos 5000 registros, se seleccionan balanceados entre registros clasificados como suicidio y registros que no. Ya cuando se tenga el modelo probado, se utilizarán todos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep only 5000 random rows balanced between the two values of the column 'class' \n",
    "# df_originales = df_originales.groupby('class', group_keys=False).apply(lambda x: x.sample(min(len(x), 2500)))\n",
    "# # df_originales = df_originales.sample(n=5000)\n",
    "\n",
    "# # Imprimir número de filas\n",
    "# print('Número de filas: ', df_originales.shape[0])\n",
    "# # Imprimir número de columnas\n",
    "# print('Número de columnas: ', df_originales.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y entendimiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de filas con valores nulos:  0\n",
      "Número de columnas con valores nulos:  0\n",
      "\n",
      "Porcentaje de completitud de las columnas: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Ver datos núlos\n",
    "print(\"\\nNúmero de filas con valores nulos: \", df_originales.isnull().any(axis=1).sum())\n",
    "print(\"Número de columnas con valores nulos: \", df_originales.isnull().any().sum())\n",
    "\n",
    "# Porcentaje de completitud\n",
    "print(f\"\\nPorcentaje de completitud de las columnas: {(1-(df_originales.isnull().any(axis=1).sum()/df_originales.shape[0]))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de filas duplicadas:  0\n",
      "Número de filas con indice duplicado:  0\n"
     ]
    }
   ],
   "source": [
    "# Ver duplicidad de datos\n",
    "print(\"\\nNúmero de filas duplicadas: \", df_originales.duplicated().sum())\n",
    "print(\"Número de filas con indice duplicado: \", df_originales['Unnamed: 0'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columna 'Unnamed: 0'\n",
    "df_originales.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# Convertir valores de la columna text a string\n",
    "df_originales['text'] = df_originales['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-suicide    110165\n",
      "suicide         85535\n",
      "Name: class, dtype: int64\n",
      "0    110165\n",
      "1     85535\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Obtener valores únicos columna 'class' \n",
    "print(df_originales['class'].value_counts())\n",
    "\n",
    "# En la columna 'class' cambiar valores 'non-suicide' por 0 y 'suicide' por 1\n",
    "df_originales['class'] = df_originales['class'].replace({'non-suicide': 0, 'suicide':1})\n",
    "\n",
    "# Obtener valores únicos columna 'class' \n",
    "print(df_originales['class'].value_counts())\n",
    "\n",
    "# Convertir la columna en numerica\n",
    "df_originales['class'] = df_originales['class'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por facilidad y tiempos de respuesta, se hace un preprocesamiento antes de separar los datos en tokens: se eliminan caracteres especiales, puntuación y números, y se convierte el texto a minúscula. Para el caso de suicidios no se considera que los números sean importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17428\\903552774.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_originales['text'] = df_originales['text'].str.replace('[^a-zA-Z ]', '')\n"
     ]
    }
   ],
   "source": [
    "# Eliminar caracteres distintos al alfabeto en la columna 'text'\n",
    "df_originales['text'] = df_originales['text'].str.replace('[^a-zA-Z ]', '')\n",
    "\n",
    "# Convertir columna text a minúsculas\n",
    "df_originales['text'] = df_originales['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos para preprocesamiento de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode(\n",
    "            'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_words_without_vowels(words):\n",
    "    \"\"\"Remove words without vowels from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if re.search('[aeiouy]', word):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def preprocessing(words):\n",
    "    # words = to_lowercase(words)\n",
    "    # words = replace_numbers(words)\n",
    "    # words = remove_punctuation(words)\n",
    "    # words = remove_non_ascii(words)\n",
    "    words = remove_words_without_vowels(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo = df_originales.copy()\n",
    "df_modelo['text'] = df_modelo['text'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo['tokens'] = df_modelo['text'].apply(word_tokenize).apply(preprocessing) \n",
    "\n",
    "# Eliminar tokens vacíos y con longitud menor a 2\n",
    "df_modelo['tokens'] = df_modelo['tokens'].apply(lambda x: [item for item in x if item != '' and (len(item) >= 2 or item == 'i')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos lematización y eliminación de prefijos y sufijos (stemming) [NORMALIZACIÓN]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    # Stem: cortar la raíz de una palabra\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems + lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60891</th>\n",
       "      <td>going to buy my first console ps what are some...</td>\n",
       "      <td>0</td>\n",
       "      <td>[going, buy, first, consol, gam, must, play, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78088</th>\n",
       "      <td>make it stop pleasei cannot do this anymore i ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[mak, stop, please, anym, much, pain, everyday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169268</th>\n",
       "      <td>i just wanted to share one of the embarrassing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[want, shar, on, embarrassingawkward, tim, lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159499</th>\n",
       "      <td>can i roast your country if willing to take so...</td>\n",
       "      <td>0</td>\n",
       "      <td>[roast, country, wil, tak, crit, pleas, nam, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82956</th>\n",
       "      <td>when the imposter when the impogster sus jdksj...</td>\n",
       "      <td>0</td>\n",
       "      <td>[impost, impogst, sus, jdksjdjdhekehdidkdjj, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  class  \\\n",
       "60891   going to buy my first console ps what are some...      0   \n",
       "78088   make it stop pleasei cannot do this anymore i ...      1   \n",
       "169268  i just wanted to share one of the embarrassing...      0   \n",
       "159499  can i roast your country if willing to take so...      0   \n",
       "82956   when the imposter when the impogster sus jdksj...      0   \n",
       "\n",
       "                                                   tokens  \n",
       "60891   [going, buy, first, consol, gam, must, play, m...  \n",
       "78088   [mak, stop, please, anym, much, pain, everyday...  \n",
       "169268  [want, shar, on, embarrassingawkward, tim, lif...  \n",
       "159499  [roast, country, wil, tak, crit, pleas, nam, c...  \n",
       "82956   [impost, impogst, sus, jdksjdjdhekehdidkdjj, s...  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplica lematización y Eliminación de Prefijos y Sufijos.\n",
    "df_modelo['tokens'] = df_modelo['tokens'].apply(stem_and_lemmatize)\n",
    "\n",
    "df_modelo.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo['tokens'] = df_modelo['tokens'].apply(\n",
    "    lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de columna 'text' para disminuir el tamaño del dataset\n",
    "df_modelo.drop('text', axis=1, inplace=True)\n",
    "\n",
    "# Eliminar filas con valores nulos\n",
    "df_modelo.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización de texto\n",
    "\n",
    "La vectorización de datos se va a realizar en los archivos específicos de cada modelo utilizando la librería sklearn.feature_extraction.text.TfidfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación de datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado exitosamente\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    filepath = Path('../data/processed_data.csv')\n",
    "    # filepath = Path('../data/processed_data_min.csv')\n",
    "    df_modelo.to_csv(filepath, index=False)\n",
    "    print(\"Archivo guardado exitosamente\")\n",
    "except Exception as e:\n",
    "    print(\"Error al guardar el archivo: \" + str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
