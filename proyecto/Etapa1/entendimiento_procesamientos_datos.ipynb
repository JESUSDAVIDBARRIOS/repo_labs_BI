{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías y lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 25) # Número máximo de columnas a mostrar\n",
    "pd.set_option('display.max_rows', 50) # Numero máximo de filas a mostar\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "np.random.seed(3301)\n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sns \n",
    "\n",
    "# Matplolib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# librería Natural Language Toolkit, usada para trabajar con texts\n",
    "import unicodedata\n",
    "import string\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import inflect\n",
    "import contractions\n",
    "import nltk\n",
    "# Punkt permite separar un text en frases.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas:  195700\n",
      "Número de columnas:  3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21738</th>\n",
       "      <td>135566</td>\n",
       "      <td>AAAA I'm literally the stupidest person in exi...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26627</th>\n",
       "      <td>17383</td>\n",
       "      <td>Is it just me or is the sound of rain One of t...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135198</th>\n",
       "      <td>36707</td>\n",
       "      <td>How close i amam tired of living, tired of bei...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140848</th>\n",
       "      <td>186151</td>\n",
       "      <td>Guess who kissed a girl? Not me.\\n\\n&amp;amp;#x200...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79670</th>\n",
       "      <td>255320</td>\n",
       "      <td>Should I delete my Reddit account? I mean shou...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "21738       135566  AAAA I'm literally the stupidest person in exi...   \n",
       "26627        17383  Is it just me or is the sound of rain One of t...   \n",
       "135198       36707  How close i amam tired of living, tired of bei...   \n",
       "140848      186151  Guess who kissed a girl? Not me.\\n\\n&amp;#x200...   \n",
       "79670       255320  Should I delete my Reddit account? I mean shou...   \n",
       "\n",
       "              class  \n",
       "21738   non-suicide  \n",
       "26627   non-suicide  \n",
       "135198      suicide  \n",
       "140848  non-suicide  \n",
       "79670   non-suicide  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_route = '../data/SuicidiosProyecto.csv'\n",
    "df_originales = pd.read_csv(db_route, encoding = 'ISO-8859-1')\n",
    "\n",
    "# Imprimir número de filas\n",
    "print('Número de filas: ', df_originales.shape[0])\n",
    "# Imprimir número de columnas\n",
    "print('Número de columnas: ', df_originales.shape[1])\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_originales.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y entendimiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de filas con valores nulos:  0\n",
      "Número de columnas con valores nulos:  0\n",
      "\n",
      "Porcentaje de completitud de las columnas: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Ver datos núlos\n",
    "print(\"\\nNúmero de filas con valores nulos: \", df_originales.isnull().any(axis=1).sum())\n",
    "print(\"Número de columnas con valores nulos: \", df_originales.isnull().any().sum())\n",
    "\n",
    "# Porcentaje de completitud\n",
    "print(f\"\\nPorcentaje de completitud de las columnas: {(1-(df_originales.isnull().any(axis=1).sum()/df_originales.shape[0]))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de filas duplicadas:  0\n",
      "Número de filas con indice duplicado:  0\n"
     ]
    }
   ],
   "source": [
    "# Ver duplicidad de datos\n",
    "print(\"\\nNúmero de filas duplicadas: \", df_originales.duplicated().sum())\n",
    "print(\"Número de filas con indice duplicado: \", df_originales['Unnamed: 0'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columna 'Unnamed: 0'\n",
    "df_originales.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# Convertir valores de la columna text a string\n",
    "df_originales['text'] = df_originales['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-suicide    110165\n",
      "suicide         85535\n",
      "Name: class, dtype: int64\n",
      "0    110165\n",
      "1     85535\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Obtener valores únicos columna 'class' \n",
    "print(df_originales['class'].value_counts())\n",
    "\n",
    "# En la columna 'class' cambiar valores 'non-suicide' por 0 y 'suicide' por 1\n",
    "df_originales['class'] = df_originales['class'].replace({'non-suicide': 0, 'suicide':1})\n",
    "\n",
    "# Obtener valores únicos columna 'class' \n",
    "print(df_originales['class'].value_counts())\n",
    "\n",
    "# Convertir la columna en numerica\n",
    "df_originales['class'] = df_originales['class'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por facilidad y tiempos de respuesta, se hace un preprocesamiento antes de separar los datos en tokens: se eliminan caracteres especiales y números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21212\\1514716280.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_originales['text'] = df_originales['text'].str.replace('[^a-zA-Z ]', '')\n"
     ]
    }
   ],
   "source": [
    "# Eliminar caracteres distintos al alfabeto en la columna 'text'\n",
    "df_originales['text'] = df_originales['text'].str.replace('[^a-zA-Z ]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos para preprocesamiento de text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode(\n",
    "            'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo = df_originales.copy()\n",
    "df_modelo['text'] = df_modelo['text'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo['tokens'] = df_modelo['text'].apply(word_tokenize).apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos lematización y eliminación de prefijos y sufijos (stemming) [NORMALIZACIÓN]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    # Stem: cortar la raíz de una palabra\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems + lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica lematización y Eliminación de Prefijos y Sufijos.\n",
    "df_modelo['tokens'] = df_modelo['tokens'].apply(stem_and_lemmatize)\n",
    "\n",
    "df_modelo.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo['tokens'] = df_modelo['tokens'].apply(\n",
    "    lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización de texto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
